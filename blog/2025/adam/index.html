<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> A mathematical intuition behind adam | Bhaswata Choudhury </title> <meta name="author" content="Bhaswata Choudhury"> <meta name="description" content="Understanding the mathematical foundations of the Adam optimizer, including the necessity and derivation of bias correction"> <meta name="keywords" content="AI, Deep Learning, AI Agents, Generative AI"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9C%A8&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://bhaswata08.github.io/blog/2025/adam/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Bhaswata</span> Choudhury </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/reading_list/">bookshelf </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">A mathematical intuition behind adam</h1> <p class="post-meta"> Created on December 02, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/optimizers"> <i class="fa-solid fa-hashtag fa-sm"></i> Optimizers</a>   ·   <a href="/blog/category/optimizers"> <i class="fa-solid fa-tag fa-sm"></i> Optimizers</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="prerequisites">Prerequisites</h1> <p>This blog assumes familiarity with the following topics:</p> <ul> <li>Stochastic Gradient Descent and its limitations</li> <li>Adagrad and its limitations</li> <li>RMSProp and its limitations</li> </ul> <h1 id="notation">Notation</h1> <p>Throughout this post, we use the following notation:</p> <ul> <li>$\theta_t$ or $w_t$: model parameters at time step $t$</li> <li>$g_t$: gradient of the loss function at time step $t$</li> <li>$m_t$: first moment estimate (momentum term)</li> <li>$v_t$: second moment estimate (adaptive learning rate term)</li> <li>$\beta_1$: exponential decay rate for first moment (typically 0.9)</li> <li>$\beta_2$: exponential decay rate for second moment (typically 0.999)</li> <li>$\eta$: learning rate</li> <li>$\epsilon$: small constant for numerical stability (typically $10^{-8}$)</li> </ul> <h1 id="introduction">Introduction</h1> <p>Adam (Adaptive Moment Estimation) is a hybrid optimizer that combines momentum-based gradient descent with the adaptive learning rate concept from RMSProp. It maintains exponentially weighted moving averages of both the gradient (first moment) and the squared gradient (second moment), along with bias correction to account for initialization at zero.</p> <p>Let’s first examine a flawed version of Adam without bias correction, then understand why bias correction is necessary.</p> <h2 id="adam-without-bias-correction-flawed-version">Adam without Bias Correction (Flawed Version)</h2> <p>The update rule is: \(\theta_{t} = \theta_{t-1} - \frac{\eta}{\sqrt{v_t} + \epsilon} m_t\)</p> <p>where:</p> <ul> <li>$m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t$ (first moment: momentum term)</li> <li>$v_t = \beta_2 v_{t-1} + (1-\beta_2) g_t^2$ (second moment: adaptive learning rate from RMSProp)</li> </ul> <p>Note that:</p> <ul> <li>$\beta_1$ controls the momentum (typically 0.9)</li> <li>$\beta_2$ controls the adaptive learning rate adjustment (typically 0.999)</li> <li>Both $m_0 = 0$ and $v_0 = 0$</li> </ul> <h2 id="testing-the-flawed-optimizer">Testing the Flawed Optimizer</h2> <p>Testing our optimizer on the Ackley function, a challenging loss landscape defined by: \(f(\mathbf{x}) = -a \exp \left( -b \sqrt{\frac{1}{d} \sum_{i=1}^{d} x_i^2} \right) - \exp \left( \frac{1}{d} \sum_{i=1}^{d} \cos(c x_i) \right) + a + \exp(1)\)</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/adam_ackley_landscape.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""></video> </figure> </div> </div> <div class="caption"> Adam without bias correction gets stuck in a local minimum of the Ackley function </div> <h3 id="observations">Observations</h3> <p>The optimizer gets stuck early in training due to severely underpowered updates. The parameter updates are too small to escape shallow local minima, even though the gradients suggest the optimizer should move more aggressively. This happens because $m_t$ and $v_t$ are biased toward zero in the early iterations.</p> <h1 id="bias-correction">Bias Correction</h1> <h2 id="why-bias-correction-is-necessary">Why Bias Correction is Necessary</h2> <p>Recall that both moments are initialized at zero:</p> <ul> <li>$m_0 = 0$</li> <li>$v_0 = 0$</li> </ul> <p>We can expand the recursive definitions to see the full history: \(m_t = (1-\beta_1) \sum_{\tau=1}^t \beta_1^{t-\tau} g_{\tau}\) \(v_t = (1-\beta_2) \sum_{\tau=1}^t \beta_2^{t-\tau} g_{\tau}^2\)</p> <h3 id="the-problem-with-early-iterations">The Problem with Early Iterations</h3> <p>The sum in $m_t$ is multiplied by $(1-\beta_1)$, and each past gradient is exponentially down-weighted by $\beta_1^{t-\tau}$.</p> <p>When $\beta_1$ is close to 1 (e.g., 0.9), the factor $(1-\beta_1)$ is small (0.1), making $m_t$ significantly smaller than it should be, especially for small $t$. For example:</p> <ul> <li>At $t=1$: $m_1 = (1-\beta_1) g_1 = 0.1 \cdot g_1$</li> <li>At $t=2$: $m_2 = 0.1 \cdot (0.9 \cdot g_1 + g_2) = 0.09 g_1 + 0.1 g_2$</li> </ul> <p>The estimates are biased toward zero.</p> <p>Similarly, $v_t$ suffers from the same bias. However, because $v_t$ appears in the denominator under a square root in the Adam update rule, while $m_t$ is in the numerator, the low momentum term $m_t$ dominates and causes the updates to be too small early in training.</p> <h2 id="deriving-the-bias-correction-factor">Deriving the Bias Correction Factor</h2> <p>We want to find a correction factor that makes our biased estimates unbiased in expectation.</p> <h3 id="derivation-for-first-moment-m_t">Derivation for First Moment $m_t$</h3> <p>Starting with the expanded form: \(m_t = (1-\beta_1) \sum_{\tau=1}^t \beta_1^{t-\tau} g_{\tau}\)</p> <p>Taking expectations and assuming <strong>stationarity</strong> (i.e., the true expected gradient is constant: $\mathbb{E}[g_\tau] = \mathbb{E}[g]$ for all $\tau$):</p> \[\begin{align*} \mathbb{E}[m_t] &amp;= \mathbb{E}\left[ (1-\beta_1) \sum_{\tau=1}^t \beta_1^{t-\tau} g_{\tau} \right] \\ &amp;= (1-\beta_1) \sum_{\tau=1}^t \beta_1^{t-\tau} \mathbb{E}[g_{\tau}] \\ &amp;= (1-\beta_1) \mathbb{E}[g] \sum_{\tau=1}^t \beta_1^{t-\tau} \quad &amp;\text{(stationarity assumption)} \\ &amp;= (1-\beta_1) \mathbb{E}[g] \cdot (\beta_1^{t-1} + \beta_1^{t-2} + \cdots + \beta_1^0) \\ &amp;= (1-\beta_1) \mathbb{E}[g] \cdot \frac{1-\beta_1^t}{1-\beta_1} \quad &amp;\text{(geometric series)} \\ &amp;= \mathbb{E}[g] \cdot (1-\beta_1^t) \end{align*}\] <p><strong>Note on the stationarity assumption</strong>: We assume that even though the optimizer is moving and the noisy gradient measurements are changing, the expected value of the gradient distribution remains constant across time steps. This is a reasonable approximation, especially in the early stages of training.</p> <p>Therefore: \(\mathbb{E}[m_t] = (1-\beta_1^t) \mathbb{E}[g]\)</p> <p>To obtain an unbiased estimate, we divide by $(1-\beta_1^t)$: \(\mathbb{E}\left[ \frac{m_t}{1-\beta_1^t} \right] = \mathbb{E}[g]\)</p> <p>Defining the bias-corrected first moment: \(\hat{m}_t = \frac{m_t}{1-\beta_1^t}\)</p> <p>We now have $\mathbb{E}[\hat{m}_t] = \mathbb{E}[g]$</p> <h3 id="derivation-for-second-moment-v_t">Derivation for Second Moment $v_t$</h3> <p>Following identical reasoning: \(\mathbb{E}[v_t] = (1-\beta_2^t) \mathbb{E}[g^2]\)</p> <p>Defining the bias-corrected second moment: \(\hat{v}_t = \frac{v_t}{1-\beta_2^t}\)</p> <p>We have $\mathbb{E}[\hat{v}_t] = \mathbb{E}[g^2]$, which is also unbiased.</p> <h2 id="complete-adam-algorithm">Complete Adam Algorithm</h2> <p>The final Adam update rule with bias correction: \(\theta_t = \theta_{t-1} - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t\)</p> <p>where:</p> <ul> <li>$m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t$</li> <li>$v_t = \beta_2 v_{t-1} + (1-\beta_2) g_t^2$</li> <li>$\hat{m}_t = \frac{m_t}{1-\beta_1^t}$</li> <li>$\hat{v}_t = \frac{v_t}{1-\beta_2^t}$</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/adam_corrected_ackley.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""></video> </figure> </div> </div> <div class="caption"> Adam with bias correction successfully converges to a good minimum </div> <h2 id="intuitive-understanding-of-bias-correction">Intuitive Understanding of Bias Correction</h2> <p>The correction factor $\frac{1}{1-\beta_1^t}$ acts as a <strong>self-adjusting boost</strong> for the momentum term:</p> <ul> <li> <strong>Early iterations</strong> (small $t$): When $\beta_1 = 0.9$ and $t = 1$, we have $\beta_1^t = 0.9$, so the correction factor is $\frac{1}{1-0.9} = 10$. This provides a significant boost.</li> <li> <strong>Later iterations</strong> (large $t$): As $t \to \infty$, $\beta_1^t \to 0$, so the correction factor approaches $\frac{1}{1-0} = 1$, meaning no correction is needed.</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/adam_correction_factor-480.webp 480w,/assets/img/adam_correction_factor-800.webp 800w,/assets/img/adam_correction_factor-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/adam_correction_factor.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Scaling of correction factor with respect to epochs (iterations) </div> <h3 id="numerical-example">Numerical Example</h3> <p>Let’s see the effect with $\beta_1 = 0.9$:</p> <table> <thead> <tr> <th>Iteration $t$</th> <th>$\beta_1^t$</th> <th>Correction Factor $\frac{1}{1-\beta_1^t}$</th> </tr> </thead> <tbody> <tr> <td>1</td> <td>0.9</td> <td>10.0</td> </tr> <tr> <td>2</td> <td>0.81</td> <td>5.26</td> </tr> <tr> <td>3</td> <td>0.729</td> <td>3.69</td> </tr> <tr> <td>5</td> <td>0.59</td> <td>2.44</td> </tr> <tr> <td>10</td> <td>0.35</td> <td>1.54</td> </tr> <tr> <td>20</td> <td>0.12</td> <td>1.14</td> </tr> <tr> <td>50</td> <td>0.005</td> <td>1.01</td> </tr> </tbody> </table> <p>The correction factor naturally decays, providing a strong boost when needed (early training) and minimal interference when the estimates have stabilized (later training).</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/Adadelta/">A mathematical understanding of AdaDelta Optimzer</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/Categorical_classification_of_optimizers/">Categorizing Deep Learning Optimizers by Modification Type</a> </li> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Bhaswata Choudhury. Last updated: December 03, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>