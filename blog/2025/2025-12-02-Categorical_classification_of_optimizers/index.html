<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h1 id="introduction">Introduction</h1> <p>All deep learning optimizers fundamentally modify the <strong>Stochastic Gradient Descent (SGD)</strong> update rule to improve convergence speed and stability.</p> <p>The base equation for SGD is:</p> \[w_t = w_{t-1} -\eta_{eff} \cdot \nabla w_t\] <p>Optimizers introduce complexity by modifying the <strong>effective learning rate term</strong> ($\eta_{eff}$) and/or the <strong>gradient direction term</strong> ($\nabla_{eff}$).</p> <h2 id="category-1-directionvelocity-modifiers-momentum">Category 1: Direction/Velocity Modifiers (Momentum)</h2> <p>These optimizers focus on modifying the <strong>effective gradient direction</strong> ($\nabla_{eff}$) via history accumulation (momentum) while maintaining a <strong>fixed global learning rate</strong> ($\eta$).</p> <ul> <li>SGD</li> <li>Momentum based Gradient Descent</li> <li>Nesterov Accelerated Gradient Descent</li> </ul> <h2 id="category-2-adaptive-learning-rate-modifiers">Category 2: Adaptive Learning Rate Modifiers</h2> <p>These optimizers focus on making the <strong>effective learning rate</strong> ($\eta_{eff}$) per-parameter and inversely proportional to the history of squared gradients. They do <strong>not</strong> use momentum accumulation.</p> <ul> <li>AdaGrad</li> <li>RMSProp</li> <li>AdaDelta j</li> </ul> <h3 id="category-3-hybrid-momentum--adaptive-eta">Category 3: Hybrid (Momentum + Adaptive $\eta$)</h3> <p>These methods combine the direction smoothing of Momentum (Category 1) with the per-parameter scaling of Adaptive $\eta$ (Category 2). They are the default choice for many modern applications.</p> <ul> <li>Adam</li> <li>AdamW</li> <li>NAdam</li> </ul> </body></html>