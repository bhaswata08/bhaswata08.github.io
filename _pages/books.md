---
layout: book-shelf
title: bookshelf
permalink: /reading_list/
nav: true
collection: reading_list
---

## Papers that I am reading, have read, or will read

### Reading queue

- [] [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- [] []

## Optimizers sequence to study

- [x] Newton Ralphson
- [x] SGD
- [x] Momentum based SGD
- [x] Nesterov Based SGD
- [x] AdaGrad
- [x] RMSProp
- [x] AdaDelta
- [x] Adam
- [] AdaMax and MaxProp
- [] Nadam
- [] AMSGrad
- [] AdaBound/AMSBound
- [] AdamW
- [] RAdam
- [] Lookahead
- [] Ranger
- [] Layer-wise adaptive moments(LAMB)
- [] Shampoo/Distributed Shampoo
- [] Sophia
- [] Muon
- [] Schedule free optimizers
