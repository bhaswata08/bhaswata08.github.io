---
layout: post
title: A mathematical understanding of AdaDelta Optimzer
date: 2025-12-01 17:07
description: Post describing the optimization of gradients with AdaDelta Optimizer
tags: Optimizers
categories: Optimizers
chart:
  plotly: false
---

# Introduction

AdaDelta is an adaptive learning rate optimization algorithm that addresses the monotonic decay issue of **AdaGrad** and offers a more robust step-size mechanism than **RMSProp** by eliminating the need for a global learning rate η.

# Prerequisites

This blog assumes certain assumptions and a conceptual understanding of the following topics:

- Stochastic Gradient descent and its limitations
- Adagrad and its limitations
- RMSProp and its limitations

# Core Problems

| Optimizer | Limitation                                                                                             |
| --------- | ------------------------------------------------------------------------------------------------------ |
| AdaGrad   | Learning rate η decays monotonically → kills off updates too early.                                    |
| RMSProp   | Still depends on an initial, fixed global η. Issue of oscillations as number of epochs/steps increase. |

# Goals

As per AdaGrad and RMSProp, our Goal remains widely the same:

- In steep regions ($\nabla w_t$ is high), take a low step size ($\Delta w_t$). (Don't overshoot.)
- In flat regions ($\nabla w_t$ is low), take a high step size (\Delta w_t$). (Learn quickly.)

# Terminology

The weight update $w_t$ at timestep $t$ is:

$$w_t=w_{t-1}+\Delta w_t$$

Where the step size $\Delta w_t$ is:

$$\Delta w_t = -\eta_{eff} \cdot \nabla w_t$$

In AdaDelta, the effective learning rate $\eta_{eff}$ is defined as the ratio of exponentially decaying averages (EDAs) calculated over the past steps and the current slopes.

$$\eta_{eff} = \sqrt{\frac{E[\Delta w^2]_{t-1}+\epsilon}{E[\nabla w_t^2]+\epsilon}}$$

Where,

- $E[\Delta w^2]_{t-1}$ is the EDA of **past** squared steps ($\Delta w$).
- $E[\nabla w^2]_{t}$ is the EDA of **current** squared gradients ($\Delta w$).
- $\epsilon$ is a small constant kept for numerical stability and avoiding division by zero error.

* **Step EDA:** $E\left[ \Delta w^2 \right]\_t = \beta E\left[ \Delta w^2 \right]\_{t-1}+ (1-\beta) (\Delta w\_t)^2$
* **Gradient EDA:** $E\left[ \nabla w^2 \right]\_t = \beta E\left[ \nabla w^2 \right]\_{t-1} + (1-\beta) (\nabla w\_t)^2$
* $\beta$ is the rate of decay of old the effect of parameters at previous timesteps

$$\eta_{eff} \propto \frac{\Delta w_{t-1}}{\nabla w_t}$$

$$\eta_{eff} \propto \frac{\text{step at timestep t-1}}{\text{slope at timestep t}}$$

# Conceptual understanding

## Making sense of the ratio

The core of AdaDelta is the proportional relationship:

$$\Delta w_t \propto \frac{\sqrt{E[\Delta w^2]_{t-1}}}{\sqrt{E[\nabla w^2]_t}} \cdot \nabla w_t$$

### 1. Self correction

- **In a steep region (High $\nabla w$):** The denominator, $\sqrt{E[\nabla w^2]_t}$, will be **high**. This makes the ratio $\eta_{eff}$ **low**. A low $\eta_{eff}$ ensures the step $\Delta w_t$ remains small, preventing an overshoot.
- **In a flat region (Low $\nabla w$):** The denominator, $\sqrt{E[\nabla w^2]_t}$, will be **low**. This makes the ratio $\eta_{eff}$ **high**. A high $\eta_{eff}$ allows the algorithm to take large steps, accelerating convergence.

### 2. Dimensional consistency: Why the ratio of step over slope

The standard weight update equation is analogous to calculating a change in position ($\Delta x$) given a velocity ($v$) and a duration ($\nabla t$).

$$v_t = v_{t-1} + \underbrace{\eta \cdot (-\nabla w_t)}_{v \cdot \Delta t}$$

- **Weight ($w$ or $x$):** Has a dimension, say **[Position/Weight]** (e.g., meters $M$).
- **Gradient ($\nabla w$):** Is the derivative of Loss ($L$) w.r.t. Weight ($M$). It has the dimension **[Slope]** (e.g., $\frac{\text{Loss}}{\text{Velocity}}$ or $\frac{L}{M}$).
  Now,

$$\Delta w_t = \eta \cdot \nabla w_t$$

$$\begin{array}{ccc} \underbrace{[\Delta w]}_{[\mathbf{M}]} & = & \underbrace{[\eta]}_{?} \cdot \underbrace{[\nabla w]}_{\left[\frac{L}{M}\right]} \end{array}$$

For the dimensions to be consistent, the learning rate $\eta$ must have the dimension $\left[\frac{M^2}{L}\right]$, analogous to $\left[\frac{\text{Position}^2}{\text{Loss}}\right]$. This manual $\eta$ acts as a scaling factor to balance the units, and its correct value is hard to guess.

AdaDelta replaces $\eta$ with an $\eta_{eff}$ derived from the ratio of two exponentially decaying averages: the $\text{RMS}(\text{Step})$ and the $\text{RMS}(\text{Gradient})$.

$$\eta_{eff} = \sqrt{\frac{E[\Delta w^2]_{t-1}}{E[\nabla w^2]_t}}$$

Let's check the dimension of this ratio:

- The term $\mathbf{E[\Delta w^2]}$ (EDA of squared steps) has dimension: $\text{[Position]}^2 \implies [M^2]$.
- The term $\mathbf{E[\nabla w^2]}$ (EDA of squared gradients) has dimension: $\left[\frac{\text{Loss}}{\text{Position}}\right]^2 \implies \left[\frac{L^2}{M^2}\right]$.
  Plugging these into the $\eta_{eff}$ formula:

$$\begin{array}{ccc} \underbrace{[\eta_{eff}]}_{?} & = & \sqrt{\frac{\underbrace{[E[\Delta w^2]]}_{[M^2]}}{\underbrace{[E[\nabla w^2]]}_{\left[\frac{L^2}{M^2}\right]}}} = \sqrt{\frac{M^2}{L^2/M^2}} = \sqrt{\frac{M^4}{L^2}} = \left[\frac{\mathbf{M}^2}{\mathbf{L}}\right] \end{array}$$

The effective learning rate $\eta_{eff}$ constructed by AdaDelta naturally has the exact dimension $\left[\frac{M^2}{L}\right]$ that is required for the weight update equation to be dimensionally correct.

#### Analogy with respect to energy

A better conceptual analogy of the gradient descent can be thought of a function of **Energy(E)**, **Position(P)** and **Force(E/P)**.

In physics, the conservative force $F$ is defined as the negative gradient of potential energy $E_p$.

$$F(x) = -\nabla E_p(x)$$

This means the force always points in the direction of the **steepest decrease** in potential energy. A ball on a hill always rolls in the direction of the greatest slope downward.
Similarly, Gradient Descent updates the weights w using the **negative gradient** of the Loss function L.

$$\Delta w = -\eta \nabla w_t$$

Readers can do similar dimensional analysis to arrive at the same conclusion.

## Why the $t−1$ Lag?

Recall that the formula for AdaDelta as:

$$\Delta w_t = -\underbrace{\sqrt{\frac{E[\Delta w^2]_{t-1} + \epsilon}{E[\nabla w^2]_t + \epsilon}}}_{\eta_{eff}} \cdot \nabla w_t$$

We use the **past steps** $E[\Delta w^2]_{t-1}$ in the numerator, but the **current gradient** $E[\nabla w^2]_t$ in the denominator.

#### Avoiding circular dependency

If we were to use the current step $E[\Delta w^2]_t$ in the numerator, the formula would become circular:

$$\Delta w_t \propto \sqrt{\frac{E[\Delta w^2]_{t}}{E[\nabla w^2]_t}} \cdot \nabla w_t$$

The term $E[\Delta w^2]_t$ itself depends on the current step $\Delta w_t$, as defined by the EDA:

$$E[\Delta w^2]_t = \beta E[\Delta w^2]_{t-1} + (1-\beta) (\Delta w_t)^2$$

Substituting this into the update rule:

$$\Delta w_t = -\sqrt{\frac{\beta E[\Delta w^2]_{t-1} + (1-\beta) (\Delta w_t)^2 + \epsilon}{E[\nabla w^2]_t + \epsilon}} \cdot \nabla w_t$$

We are trying to calculate the value of $\Delta w_t$, but the equation requires $\Delta w_t$ to be on both sides! This creates a circular dependency that cannot be solved easily or efficiently in an iterative update.

**The Solution:** The only way to break this cycle and define $\Delta w_t$ explicitly is to rely on the results of the previous time step ($t-1$) for the numerator. This ensures that $\eta_{eff}$ is always a function of known, computed values when calculating the current $\Delta w_t$.

#### Conceptual effect: Anticipatory self-regulation

- **Numerator ($\mathbf{t-1}$):** $\text{RMS}(\Delta w)_{t-1}$ measures the **historical displacement** or the **"Size of Past Movements"**. If the system has been shifting its position by large amounts (large $\Delta w_{t-1}$), this term is large.
- **Denominator ($\mathbf{t}$):** $\text{RMS}(\nabla w)_{t}$ measures the **Current Force** ($\mathbf{F}$), as the gradient of the Loss (Energy) is Force.

The ratio $\eta_{eff} \propto \frac{\text{Past Speed}}{\text{Current Force}}$ acts as a **self-regulating brake/accelerator**:

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/Adadelta.jpg" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
   Visulization of the well.
    
</div>

1. Approaching a wall
   - **Past Speed ($t-1$) is High:** The system has been taking large movements ($\Delta w$) because the energy landscape was flat (low force). The numerator is large.
   - **Current Force ($t$) is High:** The system just hit the steep wall. The Force ($\nabla w_t$) is suddenly very large. The denominator is large.
   - The overwhelming increase in the Denominator (Force) is dominant. $\eta_{eff}$ drops sharply, resulting in a small $\Delta w_t$ (small displacement) to prevent the system from overshooting the minimum. The history of large movements does not overrule the immediate danger presented by the current powerful force.
2. Stuck in a flat valley: - **Past Speed ($t-1$) is Low:** Movements were small. The numerator is small. - **Current Force ($t$) is Low:** The Force ($\nabla w_t$) is near zero. The denominator is very small. - Since both the numerator and denominator are small, the resulting ratio $\eta_{eff}$ can **stay moderate or high**. This allows the optimizer to maintain a decent displacement size, unlike the original AdaGrad, which would have accumulated small forces in the denominator, resulting in a tiny, ineffective step size.
   The $t-1$ term for steps ensures that the adaptation is smooth and takes historical context into account, while the $t$ term for the gradient ensures immediate reaction to the current local landscape.

# Adagrad vs RMSProp vs AdaDelta

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include video.liquid path="assets/video/adagrad_rmsprop_adadelta_1.mp4" class="img-fluid rounded z-depth-1" controls=true autoplay=false %}
    </div>
</div>
<div class="caption">
    Visualization of AdaGrad vs RMSProp vs AdaDelta
</div>

# Key Takeaways

**What makes AdaDelta different:**

- **AdaGrad**: Accumulates all gradients → $\eta$ decays to zero
- **RMSProp**: Exponential averaging fixes decay → but still needs manual $\eta$
- **AdaDelta**: Ratio of two EMAs → no manual $\eta$ at all

The ratio $\frac{\text{RMS}(\text{updates})}{\text{RMS}(\text{gradients})}$ automatically scales steps appropriately for each parameter.

# References

- [ADADELTA: An Adaptive Learning Rate Method](https://arxiv.org/abs/1212.5701)
- [AdaDelta](https://youtu.be/EcX0rqjHR9k?si=HpENgCvfBNtjL_lQ)
