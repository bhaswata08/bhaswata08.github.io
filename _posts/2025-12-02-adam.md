---
layout: post
title: A mathematical intuition behind adam
date: 2025-12-02 17:38
description: TODO
tags: Optimizers
categories: Optimizers
chart:
  plotly: false
---

# Introduction

Adam is a hybrid optimizer which incorporates concepts of momentum with Exponentially Weighted Moving Average(EWMA) concept introduced with RMSProp alongside bias correction.

For the purpose of explanation, lets delve into explaining a flawed version of Adam without bias correction and forget the concept of bias correction.

Formula for Adam (without bias correction):

$$w_{t} = w_{t-1} - \frac{\eta}{\sqrt{v_t} + \epsilon} m_t$$

where,

- $v_t = \beta_1 v_{t-1} + (1-\beta_1) (\nabla w_t)^2$ or the learning rate modification from RMSProp
- $m_t = \beta_2 m_{t-1} + (1-\beta_2) \nabla w_t$ or momentum term from momentum based gradient descent

## Let's put our optimizer to test
