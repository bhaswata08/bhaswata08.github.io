---
layout: post
title: A mathematical intuition behind adam
date: 2025-12-02 17:38
description: Understanding the mathematical foundations of the Adam optimizer, including the necessity and derivation of bias correction
tags: Optimizers
categories: Optimizers
chart:
  plotly: false
---

# Prerequisites

This blog assumes familiarity with the following topics:

- Stochastic Gradient Descent and its limitations
- Adagrad and its limitations
- RMSProp and its limitations

# Notation

Throughout this post, we use the following notation:

- $\theta_t$ or $w_t$: model parameters at time step $t$
- $g_t$: gradient of the loss function at time step $t$
- $m_t$: first moment estimate (momentum term)
- $v_t$: second moment estimate (adaptive learning rate term)
- $\beta_1$: exponential decay rate for first moment (typically 0.9)
- $\beta_2$: exponential decay rate for second moment (typically 0.999)
- $\eta$: learning rate
- $\epsilon$: small constant for numerical stability (typically $10^{-8}$)

# Introduction

Adam (Adaptive Moment Estimation) is a hybrid optimizer that combines momentum-based gradient descent with the adaptive learning rate concept from RMSProp. It maintains exponentially weighted moving averages of both the gradient (first moment) and the squared gradient (second moment), along with bias correction to account for initialization at zero.

Let's first examine a flawed version of Adam without bias correction, then understand why bias correction is necessary.

## Adam without Bias Correction (Flawed Version)

The update rule is:
$$\theta_{t} = \theta_{t-1} - \frac{\eta}{\sqrt{v_t} + \epsilon} m_t$$

where:

- $m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t$ (first moment: momentum term)
- $v_t = \beta_2 v_{t-1} + (1-\beta_2) g_t^2$ (second moment: adaptive learning rate from RMSProp)

Note that:

- $\beta_1$ controls the momentum (typically 0.9)
- $\beta_2$ controls the adaptive learning rate adjustment (typically 0.999)
- Both $m_0 = 0$ and $v_0 = 0$

## Testing the Flawed Optimizer

Testing our optimizer on the Ackley function, a challenging loss landscape defined by:
$$f(\mathbf{x}) = -a \exp \left( -b \sqrt{\frac{1}{d} \sum_{i=1}^{d} x_i^2} \right) - \exp \left( \frac{1}{d} \sum_{i=1}^{d} \cos(c x_i) \right) + a + \exp(1)$$

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include video.liquid path="assets/video/adam_ackley_landscape.mp4" class="img-fluid rounded z-depth-1" controls=true autoplay=false %}
    </div>
</div>
<div class="caption">
    Adam without bias correction gets stuck in a local minimum of the Ackley function
</div>

### Observations

The optimizer gets stuck early in training due to severely underpowered updates. The parameter updates are too small to escape shallow local minima, even though the gradients suggest the optimizer should move more aggressively. This happens because $m_t$ and $v_t$ are biased toward zero in the early iterations.

# Bias Correction

## Why Bias Correction is Necessary

Recall that both moments are initialized at zero:

- $m_0 = 0$
- $v_0 = 0$

We can expand the recursive definitions to see the full history:
$$m_t = (1-\beta_1) \sum_{\tau=1}^t \beta_1^{t-\tau} g_{\tau}$$
$$v_t = (1-\beta_2) \sum_{\tau=1}^t \beta_2^{t-\tau} g_{\tau}^2$$

### The Problem with Early Iterations

The sum in $m_t$ is multiplied by $(1-\beta_1)$, and each past gradient is exponentially down-weighted by $\beta_1^{t-\tau}$.

When $\beta_1$ is close to 1 (e.g., 0.9), the factor $(1-\beta_1)$ is small (0.1), making $m_t$ significantly smaller than it should be, especially for small $t$. For example:

- At $t=1$: $m_1 = (1-\beta_1) g_1 = 0.1 \cdot g_1$
- At $t=2$: $m_2 = 0.1 \cdot (0.9 \cdot g_1 + g_2) = 0.09 g_1 + 0.1 g_2$

The estimates are biased toward zero.

Similarly, $v_t$ suffers from the same bias. However, because $v_t$ appears in the denominator under a square root in the Adam update rule, while $m_t$ is in the numerator, the low momentum term $m_t$ dominates and causes the updates to be too small early in training.

## Deriving the Bias Correction Factor

We want to find a correction factor that makes our biased estimates unbiased in expectation.

### Derivation for First Moment $m_t$

Starting with the expanded form:
$$m_t = (1-\beta_1) \sum_{\tau=1}^t \beta_1^{t-\tau} g_{\tau}$$

Taking expectations and assuming **stationarity** (i.e., the true expected gradient is constant: $\mathbb{E}[g_\tau] = \mathbb{E}[g]$ for all $\tau$):

$$
\begin{align*}
\mathbb{E}[m_t] &= \mathbb{E}\left[ (1-\beta_1) \sum_{\tau=1}^t \beta_1^{t-\tau} g_{\tau} \right] \\
&= (1-\beta_1) \sum_{\tau=1}^t \beta_1^{t-\tau} \mathbb{E}[g_{\tau}] \\
&= (1-\beta_1) \mathbb{E}[g] \sum_{\tau=1}^t \beta_1^{t-\tau} \quad &\text{(stationarity assumption)} \\
&= (1-\beta_1) \mathbb{E}[g] \cdot (\beta_1^{t-1} + \beta_1^{t-2} + \cdots + \beta_1^0) \\
&= (1-\beta_1) \mathbb{E}[g] \cdot \frac{1-\beta_1^t}{1-\beta_1} \quad &\text{(geometric series)} \\
&= \mathbb{E}[g] \cdot (1-\beta_1^t)
\end{align*}
$$

**Note on the stationarity assumption**: We assume that even though the optimizer is moving and the noisy gradient measurements are changing, the expected value of the gradient distribution remains constant across time steps. This is a reasonable approximation, especially in the early stages of training.

Therefore:
$$\mathbb{E}[m_t] = (1-\beta_1^t) \mathbb{E}[g]$$

To obtain an unbiased estimate, we divide by $(1-\beta_1^t)$:
$$\mathbb{E}\left[ \frac{m_t}{1-\beta_1^t} \right] = \mathbb{E}[g]$$

Defining the bias-corrected first moment:
$$\hat{m}_t = \frac{m_t}{1-\beta_1^t}$$

We now have $\mathbb{E}[\hat{m}_t] = \mathbb{E}[g]$

### Derivation for Second Moment $v_t$

Following identical reasoning:
$$\mathbb{E}[v_t] = (1-\beta_2^t) \mathbb{E}[g^2]$$

Defining the bias-corrected second moment:
$$\hat{v}_t = \frac{v_t}{1-\beta_2^t}$$

We have $\mathbb{E}[\hat{v}_t] = \mathbb{E}[g^2]$, which is also unbiased.

## Complete Adam Algorithm

The final Adam update rule with bias correction:
$$\theta_t = \theta_{t-1} - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t$$

where:

- $m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t$
- $v_t = \beta_2 v_{t-1} + (1-\beta_2) g_t^2$
- $\hat{m}_t = \frac{m_t}{1-\beta_1^t}$
- $\hat{v}_t = \frac{v_t}{1-\beta_2^t}$

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include video.liquid path="assets/video/adam_corrected_ackley.mp4" class="img-fluid rounded z-depth-1" controls=true autoplay=false %}
    </div>
</div>
<div class="caption">
    Adam with bias correction successfully converges to a good minimum
</div>

## Intuitive Understanding of Bias Correction

The correction factor $\frac{1}{1-\beta_1^t}$ acts as a **self-adjusting boost** for the momentum term:

- **Early iterations** (small $t$): When $\beta_1 = 0.9$ and $t = 1$, we have $\beta_1^t = 0.9$, so the correction factor is $\frac{1}{1-0.9} = 10$. This provides a significant boost.
- **Later iterations** (large $t$): As $t \to \infty$, $\beta_1^t \to 0$, so the correction factor approaches $\frac{1}{1-0} = 1$, meaning no correction is needed.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/adam_correction_factor.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
   Scaling of correction factor with respect to epochs (iterations)
</div>

### Numerical Example

Let's see the effect with $\beta_1 = 0.9$:

| Iteration $t$ | $\beta_1^t$ | Correction Factor $\frac{1}{1-\beta_1^t}$ |
| ------------- | ----------- | ----------------------------------------- |
| 1             | 0.9         | 10.0                                      |
| 2             | 0.81        | 5.26                                      |
| 3             | 0.729       | 3.69                                      |
| 5             | 0.59        | 2.44                                      |
| 10            | 0.35        | 1.54                                      |
| 20            | 0.12        | 1.14                                      |
| 50            | 0.005       | 1.01                                      |

The correction factor naturally decays, providing a strong boost when needed (early training) and minimal interference when the estimates have stabilized (later training).

# References
- [ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION](https://arxiv.org/pdf/1412.6980)
- [Adam](https://youtu.be/m9g9Hij1h1A?si=1fxrwah3-HY3vTsA)
